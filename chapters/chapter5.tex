%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Master's Thesis          %	    										
% Fabian Burth, 2022-08-01 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%	

\npchapter{System Design}
This section describes the design of the \textit{Security and Compliance Data Lake}. It covers the conception of the data model, the selection of a database and the design of the API. Thereby, it especially discusses alternatives and focuses on giving detailed information about the ideas and motives that lead to specific design decisions.

\section{Requirements}
Before actually going into the details of the systems design, the requirements have to be specified, since they are at the core of every design decision.  

\subsection{Functional Requirements}
The below table \ref{Tab:Requirements} provides a condensed list of the functional requirements for the Security and Compliance Data Lake. Thereby, every requirement is described by a short and precise but abstract statement of what functionality the system must have and an additional explanation which also includes an example. There is also a column categorizing the requirements as priority 1 or 2.\par 
Priority 1 is functionality deemed necessary for a central metadata store which should solve the limitations and problems identified in the previous chapters. Furthermore, priority 1 functionality is usually functionality that has to be considered in the design process and otherwise cannot be easily added without foundational remodeling.\par
Priority 2 functionality describes convenience features which are less urgent and may easily be added later on.
\begin{xltabular}{\linewidth}{|l|X|l|}
	\hline \hline \hline \rowcolor{lightgray}\multicolumn{3}{|l|}{\cellcolor{lightgray}{\textbf{Requirements}}} \\ \hline \rowcolor{lightgray} \textbf{Ref.\#} & \textbf{Functionality} & \textbf{Prio.}\\ \hline
	\endfirsthead
	
	\hline \hline \hline \rowcolor{lightgray}\multicolumn{3}{|l|}{\cellcolor{lightgray}{\textbf{Requirements}}} \\ \hline \rowcolor{lightgray} \textbf{Ref.\#} & \textbf{Functionality} & \textbf{Prio.}\\ \hline
	\endhead
	
	\hline \multicolumn{3}{|r|}{{Continued on next page}} \\ \hline
	\endfoot
	
	\hline \caption{Requirements} \label{Tab:Requirements}
	\endlastfoot
	
	R.1 & The SCDL shall be able to consume and store metadata from multiple different data sources.\newline\newline
	The SCDL shall be able to work with any kind of metadata about software components.	Therefore, it has to be able to handle multiple different scanning tools, as well as other kinds of data sources like build tools. As an example, it might have to consume data from BDBA, Mend but perhaps also Jenkins. Thus, one has to consider that besides vulnerabilities and licenses, a variety of other metadata types may need to be added in the future. & 1\\
	\hline
	R.2 & The SCDL shall store the metadata from different data sources without aggregation\footnotemark{}.\newline\newline
	Different tools that generally serve the same purpose may provide similar information. As an example, BDBA and Mend are both SCA tools and therefore provide overlapping results. To ensure that no data is lost, this information shall not be combined and aggregated before storing.
	\footnotetext{\textit{aggregation} in this context means to merge the data about a package of a BDBA scan and a Mend scan to a single package entity instance before storing} & 1\\
	\hline
	R.3 & The SCDL shall provide the metadata from different data sources with aggregation\footnotemark[\value{footnote}].\newline\newline
	As mentioned before, to ensure no data is lost, the data from different data sources shall be stored without aggregation. Anyway, to be consumed by a user, this data shall be aggregated. As an example, when querying all packages contained in a specific resource, the result returned by the SCDL shall not contain the same package twice in different representations, if it was identified by BDBA and by Mend. Instead, it shall contain an aggregated representation of the package. Thus, some kind of aggregation layer is needed which provides transparency regarding the data sources. & 1\\
	\hline
	R.4 & The SCDL shall provide a level of aggregation\footnotemark{} to group sources and resources.\newline\newline
	\footnotetext{\textit{aggregation} in this context refers to the "whole/part" semantic of the word \cite{UML}. Thus, since resources and sources are comprised of packages, they are both aggregations of packages. On a model level, the same applies for the relationships between packages and vulnerabilities or licenses as well as between entire deployments and the deployed resources.}
	As pointed out before, one problem also with SBOMs is the disconnection of the artifact metadata and the deployment information. To bridge this gap, an additional aggregation level for grouping artifacts is necessary. As an example, this additional aggregation level shall enable to group all resources contained in a specific deployment. & 1\\
	\hline
	R.5 & The SCDL shall enable users to query the metadata on different levels of aggregation\footnotemark[\value{footnote}]\newline\newline
	As an example, a user shall be able to query for all vulnerabilities in a specific resource, thus query on the aggregate level of resources. But a user shall also be able to query for all vulnerabilities in an entire specific deployment, thus querying on the aggregate level of deployments (querying on this level of aggregation enables to answer where Log4J is deployed). & 1\\
	\hline
	R.6 & The SCDL shall enable users to perform assessments.\newline\newline
	The relevance of specific pieces of information such as vulnerabilities or licenses depends on the use case. As an example, while the internal usage of an altered OSS with a copyleft license is lawful, the distribution is not. Therefore, a possibility has to be provided to assess such pieces of information in the context of their occurrence. & 2\\
	\hline
	R.7 & The SCDL shall provide common data aggregation and filter functions for the queries.\newline\newline
	As an example, a user shall be able to filter for the vulnerability with the highest CVSS within a resource or shall be able to get the count of vulnerabilities within a resource. & 2\\
	\hline
	R.8 & The SCDL shall enable users to query the metadata in the common SBOM formats.\newline\newline
	In order to be able to fulfill governmental requirements of the executive order mentioned in the Software Bill of Materials section, the SCDL has to provide a way to to query the metadata in the common SBOM formats. As an example, a user shall be able to query the SPDX document for a specific resource. & 2\\
\end{xltabular}

So, by fulfilling this functional requirements, the Security and Compliance Data Lake would actually serve as a central application for storing and querying software metadata. Thereby, solving the problem of metadata being distributed throughout the development life cycle and bridging the gap between artifact metadata and deployment information.

\subsection{Non-functional Requirements}
Since this shall be a prototypical implementation, there is a strong focus on fulfilling the functional requirements. Thus, no concrete limits regarding performance or scalability such as a maximum response time of 5 seconds or support for up to 1000 concurrent users will be set here. Considering the novelty of the topic, there is very few reference data and therefore, such specifications would be premature. However, for a central metadata store which should prospectively power dashboard web applications for monitoring purposes, scalability and performance definitely have to be considered in design decisions already.

\section{Data Model}
The basic entities relevant in the software supply chain are artifacts, thus sources and resources, and the packages comprising these artifacts. Compliance scanners usually scan entire source code repositories or binaries. Through different methodologies, these tools detect the packages contained in these scanned artifacts. By subsequently matching these packages against different databases such as the NVD, introduced in the foundations chapter, known vulnerabilities and licenses are identified. To give a better idea of these results, figure \ref{fig:bdbaResult} in the appendix shows a snippet returned from the API of Black Duck Binary Analysis (BDBA). The results on their own are useful already and provide interesting data about the above mentioned entities. But it is still loose metadata that lacks context information such as which deployments contain the corresponding entities. Therefore, an additional entity type to conduct further grouping is required. The OCM already introduced such an entity type, the component.\par
To conclude this, from a high level perspective, the important entities are \emph{components}, \emph{sources}, \emph{resources}, \emph{packages} and the information attached to the packages such as vulnerabilities and licenses. To generalize this and abstract away from specific data sources, the entity type representing this types of metadata is called \emph{info snippet}. So these entity types are the basic building blocks for the data model. From here on, it is getting rather complex and abstract. To still keep the explanations tangible, below figure \ref{fig:DataModel} already shows an \emph{entity-relationship model (ERM)} describing the final data model. This may be used as a reference point throughout the following paragraphs discussing the design decisions leading up to the specific entities, relationships and cardinalities.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.65]{datamodel}
	\caption[Data Model]{Data Model \source{Own representation}}
	\label{fig:DataModel}
\end{figure} 

Even though one may not understand the motivation behind every element immediately, the model as a whole should feel quite familiar and intuitive by now. An important notice at this point, the data model is obviously inspired by the OCM. As mentioned above, especially the entity type \emph{component} is lend from the OCM and the previous chapter even described an integration with the OCM. But still, the Security and Compliance Data Lake is designed independent of the OCM. The underlying principles and thereby, the entity and relationship types of the OCM were merely deemed as a suitable approach for an universal data model for software metadata. Thus, in theory it is entirely possible to use a different kind of component model. As an example, if one is able to express the concept of \emph{components} and \emph{artifacts} with the means of the SPDX standard, one could use SPDX instead of OCM to provide this structural information. Or, since SPDX is not optimal for this purpose, one could create and use an own component model, as long as it has the means to express \emph{components} and \emph{artifacts} (There is generally no necessity to distinct between \emph{sources} and \emph{resources}. One could just treat \emph{sources} as \emph{resources}, at the only cost of losing the connecting information the entities.). But, to keep the explanations tangible, the following paragraphs will assume that this structural information is provided by the OCM as described in the previous chapter. \\\par 
The basic entity type \emph{component} is broken down into two distinct entity types, \emph{Component} and \emph{Component Version}. As one may immediately notice, this distinction is done for each of the basic entity types. During the initial design phase, there was a lot of discussion, whether this additional level of aggregation is really necessary. But the final conclusion was, that it always makes sense, especially in regards to extensibility and performance optimization. This will be explicitly addressed and explained in detail several times throughout this and the following section.\par 
So \emph{Component} is a purely abstract entity. Thus, as an example, in the OCM, this entity does only exist implicitly, since all \emph{Component Versions} describing the same \emph{Component} have the same globally unique name. A Component Descriptor describes a specific version of a \emph{Component} and therefore, as far as the OCM goes, a \emph{Component} does not have its own properties. In this data model, the \emph{Component} exists as an explicit entity type which may have its own properties. This properties may provide information about the semantics of this grouping such as whether this \emph{Component} describes a specific deployment or whether it describes all \emph{Components} used by a department. Thus, information that is identical for all \emph{Component Versions} and would have to be stored redundantly for each \emph{Component Version} otherwise. Naturally, there are multiple \emph{Component Versions} of each \emph{Component}, thus the (n:1)-cardinality here is self-explaining. As established by the description of the OCM and by the previously described grouping semantic of \emph{components}, a \emph{Component Version} may reference multiple other \emph{Component Versions}. As an example, a \emph{Component Version} describing a specific version of a deployment may reference multiple other \emph{Component Versions} such as \emph{Component Versions} describing specific version of a web server, a service and a database. Reciprocal, a \emph{Component Version} may of course be reference by multiple \emph{Component Versions}. As an example, a \emph{Component Version} describing a web server may be referenced by several \emph{Component Versions} describing different versions of the same deployment or entirely different deployments. Thus, this is a recursive (n:m)-relationship. As already described in the context OCM, there may also be a need to store additional occurrence specific metadata as properties of the \emph{references}. Considering the above example, such occurrence specific metadata may provide information about the usage of the web server within the deployment, hence whether it is used as a HTTP server or as a load balancer. These model elements fulfill requirement R.4 (provide an aggregation level to group sources and resources). Furthermore a \emph{Component Version} may also reference multiple \emph{Sources} and \emph{Resources}. As an example, the \emph{Resources} comprising the web server and the \emph{Sources} from which the respective \emph{Resources} were built from. As before with the recursive relationship of \emph{Component Versions}, the \emph{Sources} and \emph{Resources} may of course also be referenced by multiple \emph{Component Versions}, resulting in a (n:m)-relationship. Again, there may be a need to store additional occurrence specific metadata as properties of the \emph{references}.\par
The relationships between \emph{Source} and \emph{Source Version} as well as between \emph{Resource} and \emph{Resource Version} is similar to the relationship between \emph{Component} and \emph{Component Versions}. But the abstract \emph{Source} and \emph{Resource} entities actually do have a concrete purpose but preventing redundant storage of certain properties. In this case, these abstract entities may have properties to store \emph{triage policies}. As an example, one may store that a specific vulnerability may be ignored for the usage of \emph{Resource Versions} v1.0.0 to v1.2.3 of a respective \emph{Resource} within \emph{Component Versions} v1.4.2 to v1.4.12 of a specific \emph{Component}. This thereby fulfills the requirement R.6.\par 
At this point, there is an contradiction between the OCM and the specification of the data model. The prerequisite to being able to group all \emph{Component Versions} to a \emph{Component} is a common globally unique identifier, the component name, which allows to identify all \emph{Component Versions} as versions of a specific \emph{Component} in the first place. But as described in the OCM section, the \emph{Source Versions} and \emph{Resource Versions} only have a component version-local identity. So the same physical \emph{Resource} could be identified by a different name in each \emph{Component Version}. Thus, there is no way of identifying whether \emph{Resource Versions} referenced by two different \emph{Component Versions} are the same \emph{Resource}. In fact, even if \emph{Resource Versions} referenced by two different \emph{Component Versions} do have exactly the component version-local identity (name, version and extra identity), still the only way to determine whether they are the same \emph{Resource Version} is to calculate and compare normalized digests, since even the access properties may differ. In practice, the usage of \emph{Source Versions} and \emph{Resource Versions} is that the identity is at least component-local. Thus, \emph{Source Versions} and \emph{Resource Versions} of different \emph{Component Versions} of the same \emph{Component} with the same local identity (name, version and extra identity) can be assumed to be the same. Even more, in practice, if the name and extra identity of \emph{Source Versions} or \emph{Resource Versions} of different \emph{Component Versions} of the same \emph{Component} are the same, they may be assumed to be the same. Hence, when consuming the structural information from the OCM, the relationship between \emph{Component Version} and \emph{Source Version} and \emph{Resource Version} has an actual (1:n)-cardinality. Consequently, triaging policies on \emph{Sources} and \emph{Resources} are automatically bound to the context of a \emph{Component} and multiple \emph{Source Versions} with different identities (component name, source name, source version and extra identity) may refer to the same physical source code and multiple \emph{Resource Versions} may respectively refer to the same physical binaries. But since they do in fact refer to the same physical source code or binaries, they have the same scan results. Therefore, while this is not completely clean and satisfying, its still functional.\par 






\section{Terminology}
The System Design section makes use of a lot of heavily overloaded words which may lead to confusion and make it quite difficult to follow. To prevent this, the meaning of those ambiguous words will be specified for this context in the following:\\

\noindent
\textbf{Component:} A component is an abstract entity describing a dedicated usage context or meaning for provided software, as defined by the OCM \cite{OCMSpec}.\\
\textbf{Artifact:} An artifact is an umbrella term for sources and resources. Thus, the term refers to the actual source code or binaries.\\
\textbf{Package:} A package is a functional unit contained in an artifact. In practice, a package is usually a collection of files, forming a library which is imported in the source code. 

\section{Data Model}
The basic entities relevant in the software supply chain are artifacts, thus sources and resources, and the packages comprising these artifacts.\par
Compliance scanners usually scan entire source code repositories or binaries. Through different methodologies, these tools detect the packages contained in these scanned artifacts. By subsequently matching these packages against different databases such as the NVD introduced in the foundations chapter, known vulnerabilities and licenses are identified. To give a better idea of these results, figure \ref{fig:bdbaResult} in the appendix shows a snippet returned from the API of Black Duck Binary Analysis (BDBA).\par 
The results on their own are useful already and provide interesting data about the above mentioned entities. But it is still loose metadata that lacks context information such as which deployments contain the corresponding entities. Therefore, an additional entity to conduct further grouping is required. The OCM already introduced such an entity, the component.\\\\
To conclude this, from a high level perspective, the important entities are \emph{components}, \emph{sources}, \emph{resources}, \emph{packages} and the information attached to the packages such as vulnerabilities and licenses. To generalize this and abstract away from specific tools, the entity representing this types of metadata is called \emph{info snippet}.\par 
So these entities are the basic building blocks for the data model. From here on, it is getting rather complex. A section listing the requirements beforehand was omitted on purpose, because they would have been very abstract and hard to follow. Instead, the requirements are now mentioned along the corresponding design decisions. To keep it even more tangible, below figure \ref{fig:DataModel} shows an entity-relationship model (ERM) describing the final data model. This may now be used as an anchor point throughout the following paragraphs discussing the requirements and design decisions leading up to the specific entities, relationships and cardinalities.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.65]{datamodel}
	\caption[Data Model]{Data Model \source{Own representation}}
	\label{fig:DataModel}
\end{figure} 




\section{Database}
\section{API}
Tool agnostic, how does the api look like to abstract away from different tools

\section{Requirements}
Before actually going into the details of the systems design, the requirements have to be specified, since they are at the core of every design decision.  

\subsection{Functional Requirements}
\begin{xltabular}{\linewidth}{|l|X|l|}
	\hline \hline \hline \rowcolor{lightgray}\multicolumn{3}{|l|}{\cellcolor{lightgray}{\textbf{Requirements}}} \\ \hline \rowcolor{lightgray} \textbf{Ref.\#} & \textbf{Functionality} & \textbf{Prio.}\\ \hline
	\endfirsthead
	
	\hline \hline \hline \rowcolor{lightgray}\multicolumn{3}{|l|}{\cellcolor{lightgray}{\textbf{Requirements}}} \\ \hline \rowcolor{lightgray} \textbf{Ref.\#} & \textbf{Functionality} & \textbf{Prio.}\\ \hline
	\endhead
	
	\hline \multicolumn{3}{|r|}{{Continued on next page}} \\ \hline
	\endfoot
	
	\hline \caption{Requirements} \label{Tab:Requirements}
	\endlastfoot
	
	R.1 & The SCDL shall be able to consume and store metadata from multiple different data sources.\newline\newline
	The SCDL shall be able to work with any kind of metadata about software components.	Therefore, it has to be able to handle multiple different scanning tools, as well as other kinds of data sources like build tools. As an example, it might have to consume data from BDBA, Mend but perhaps also Jenkins. Thus, one has to consider that besides vulnerabilities and licenses, a variety of other metadata types may need to be added in the future. & 1\\
	\hline
	R.2 & The SCDL shall store the metadata from different data sources without aggregation\footnotemark{}.\newline\newline
	Different tools that generally serve the same purpose may provide similar information. As an example, BDBA and Mend are both SCA tools and therefore provide overlapping results. To ensure that no data is lost, this information shall not be combined and aggregated before storing.
	\footnotetext{\textit{aggregation} in this context means to merge the data about a package of a BDBA scan and a Mend scan to a single package entity instance before storing} & 1\\
	\hline
	R.3 & The SCDL shall provide the metadata from different data sources with aggregation\footnotemark[\value{footnote}].\newline\newline
	As mentioned before, to ensure no data is lost, the data from different data sources shall be stored without aggregation. Anyway, to be consumed by a user, this data shall be aggregated. As an example, when querying all packages contained in a specific resource, the result returned by the SCDL shall not contain the same package twice in different representations, if it was identified by BDBA and by Mend. Instead, it shall contain an aggregated representation of the package. Thus, some kind of aggregation layer is needed which provides transparency regarding the data sources. & 1\\
	\hline
	R.4 & The SCDL shall provide a level of aggregation\footnotemark{} to group sources and resources.\newline\newline
	\footnotetext{\textit{aggregation} in this context refers to the "whole/part" semantic of the word \cite{UML}. Thus, since resources and sources are comprised of packages, they are both aggregations of packages. On a model level, the same applies for the relationships between packages and vulnerabilities or licenses as well as between entire deployments and the deployed resources.}
	As pointed out before, one problem also with SBOMs is the disconnection of the artifact metadata and the deployment information. To bridge this gap, an additional aggregation level for grouping artifacts is necessary. As an example, this additional aggregation level shall enable to group all resources contained in a specific deployment. & 1\\
	\hline
	R.5 & The SCDL shall enable users to query the metadata on different levels of aggregation\footnotemark[\value{footnote}]\newline\newline
	As an example, a user shall be able to query for all vulnerabilities in a specific resource, thus query on the aggregate level of resources. But a user shall also be able to query for all vulnerabilities in an entire specific deployment, thus querying on the aggregate level of deployments (querying on this level of aggregation enables to answer where Log4J is deployed). & 1\\
	\hline
	R.6 & The SCDL shall enable users to perform assessments.\newline\newline
	The relevance of specific pieces of information such as vulnerabilities or licenses depends on the use case. As an example, while the internal usage of an altered OSS with a copyleft license is lawful, the distribution is not. Therefore, a possibility has to be provided to assess such pieces of information in the context of their occurrence. & 1\\
	\hline
	R.7 & The SCDL shall provide common data aggregation and filter functions for the queries.\newline\newline
	As an example, a user shall be able to filter for the vulnerability with the highest CVSS within a resource or shall be able to get the count of vulnerabilities within a resource. & 2\\
	\hline
	R.8 & The SCDL shall enable users to query the metadata in the common SBOM formats.\newline\newline
	In order to be able to fulfill governmental requirements of the executive order mentioned in the Software Bill of Materials section, the SCDL has to provide a way to to query the metadata in the common SBOM formats. As an example, a user shall be able to query the SPDX document for a specific resource. & 2\\
\end{xltabular}

\subsection{Non-functional Requirements}
Since this shall be a prototypical implementation, there is a strong focus on fulfilling the functional requirements. Anyway, performance definitely has to be considered in design decisions already, especially since the SCDL shall serve as the backend for a dashboard web application.
Scalable (Wie viele Daten fallen im Gardener an? Wie lange wird das gut gehen? Datenarchivierung) --> PoC aufbauen und dann nachschauen! 

~200MB/Day pro Scanner

