%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Master's Thesis          %	    										
% Fabian Burth, 2022-08-01 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%	

\npchapter{System Design}
This section describes the design of the \textit{Security and Compliance Data Lake}. It covers the conception of the data model, the selection of a database and the design of the API. Thereby, it especially discusses alternatives and focuses on giving detailed information about the ideas and motives that lead to specific design decisions.

\section{Requirements}
Before actually going into the details of the systems design, the requirements have to be specified, since they are at the core of every design decision.  

\subsection{Functional Requirements}
The below table \ref{Tab:Requirements} provides a condensed list of the functional requirements for the Security and Compliance Data Lake. Thereby, every requirement is described by a short and precise but abstract statement of what functionality the system must have and an additional explanation which also includes an example. There is also a column categorizing the requirements as priority 1 or 2.\par 
Priority 1 is functionality deemed necessary for a central metadata store which should solve the limitations and problems identified in the previous chapters. Furthermore, priority 1 functionality is usually functionality that has to be considered in the design process and otherwise cannot be easily added without foundational remodeling.\par
Priority 2 functionality describes convenience features which are less urgent and may easily be added later on.
\begin{xltabular}{\linewidth}{|l|X|l|}
	\hline \rowcolor{lightgray} \multicolumn{3}{ |l| } {\cellcolor{lightgray}{\textbf{Requirements}}}
	\\ \hline \rowcolor{lightgray}\textbf{Ref.\#} & \textbf{Functionality} & \textbf{Prio.}
	\\ \hline
	\endfirsthead
	
	\hline \rowcolor{lightgray}\multicolumn{3}{|l|}{\cellcolor{lightgray}{\textbf{Requirements}}} \\ \hline \rowcolor{lightgray} \textbf{Ref.\#} & \textbf{Functionality} & \textbf{Prio.}\\ \hline
	\endhead
	
	\hline \multicolumn{3}{|r|}{{Continued on next page}} \\ \hline
	\endfoot
	
	\hline \caption{Requirements} \label{Tab:Requirements}
	\endlastfoot
	
	R.1 & The SCDL shall be able to consume and store metadata from multiple different data sources.\newline\newline
	The SCDL shall be able to work with any kind of metadata about software components.	Therefore, it has to be able to handle multiple different scanning tools, as well as other kinds of data sources like build tools. As an example, it might have to consume data from BDBA, Mend but perhaps also Jenkins. Thus, one has to consider that besides vulnerabilities and licenses, a variety of other metadata types may need to be added in the future. & 1\\
	\hline
	R.2 & The SCDL shall store the metadata from different data sources without aggregation\footnotemark{}.\newline\newline
	Different tools that generally serve the same purpose may provide similar information. As an example, BDBA and Mend are both SCA tools and therefore provide overlapping results. To ensure that no data is lost, this information shall not be combined and aggregated before storing.
	\footnotetext{\textit{aggregation} in this context means to merge the data about a package of a BDBA scan and a Mend scan to a single package entity instance before storing} & 1\\
	\hline
	R.3 & The SCDL shall provide the metadata from different data sources with aggregation\footnotemark[\value{footnote}].\newline\newline
	As mentioned before, to ensure no data is lost, the data from different data sources shall be stored without aggregation. Anyway, to be consumed by a user, this data shall be aggregated. As an example, when querying all packages contained in a specific resource, the result returned by the SCDL shall not contain the same package twice in different representations, if it was identified by BDBA and by Mend. Instead, it shall contain an aggregated representation of the package. Thus, some kind of aggregation layer is needed which provides transparency regarding the data sources. & 1\\
	\hline
	R.4 & The SCDL shall provide a level of aggregation\footnotemark{} to group sources and resources.\newline\newline
	\footnotetext{\textit{aggregation} in this context refers to the "whole/part" semantic of the word \cite{UML}. Thus, since resources and sources are comprised of packages, they are both aggregations of packages. On a model level, the same applies for the relationships between packages and vulnerabilities or licenses as well as between entire deployments and the deployed resources.}
	As pointed out before, one problem also with SBOMs is the disconnection of the artifact metadata and the deployment information. To bridge this gap, an additional aggregation level for grouping artifacts is necessary. As an example, this additional aggregation level shall enable to group all resources contained in a specific deployment. & 1\\
	\hline
	R.5 & The SCDL shall enable users to query the metadata on different levels of aggregation\footnotemark[\value{footnote}]\newline\newline
	As an example, a user shall be able to query for all vulnerabilities in a specific resource, thus query on the aggregate level of resources. But a user shall also be able to query for all vulnerabilities in an entire specific deployment, thus querying on the aggregate level of deployments (querying on this level of aggregation enables to answer where Log4J is deployed). & 1\\
	\hline
	R.6 & The SCDL shall enable users to perform assessments.\newline\newline
	The relevance of specific pieces of information such as vulnerabilities or licenses depends on the use case. As an example, while the internal usage of an altered OSS with a copyleft license is lawful, the distribution is not. Therefore, a possibility has to be provided to assess such pieces of information in the context of their occurrence. & 2\\
	\hline
	R.7 & The SCDL shall provide common data aggregation and filter functions for the queries.\newline\newline
	As an example, a user shall be able to filter for the vulnerability with the highest CVSS within a resource or shall be able to get the count of vulnerabilities within a resource. & 2\\
	\hline
	R.8 & The SCDL shall enable users to query the metadata in the common SBOM formats.\newline\newline
	In order to be able to fulfill governmental requirements of the executive order mentioned in the Software Bill of Materials section, the SCDL has to provide a way to to query the metadata in the common SBOM formats. As an example, a user shall be able to query the SPDX document for a specific resource. & 2\\
\end{xltabular}

So, by fulfilling this functional requirements, the Security and Compliance Data Lake would actually serve as a central application for storing and querying software metadata. Thereby, solving the problem of metadata being distributed throughout the development life cycle and bridging the gap between artifact metadata and deployment information.

\subsection{Non-functional Requirements}
Since this shall be a prototypical implementation, there is a strong focus on fulfilling the functional requirements. Thus, no concrete limits regarding performance or scalability such as a maximum response time of 5 seconds or support for up to 1000 concurrent users will be set here. Considering the novelty of the topic, there is very few reference data and therefore, such specifications would be premature. However, for a central metadata store which should prospectively power dashboard web applications for monitoring purposes, scalability and performance definitely have to be considered in design decisions already.

\section{Data Model}
The basic entities relevant in the software supply chain are artifacts, thus sources and resources, and the packages comprising these artifacts. The definition of sources and resources is still the same as introduced in the OCM section. Resources are capable of doing something and are usually executables or OCI Images. Sources are the code the resources are built from. Compliance scanners usually scan entire source code repositories or binaries. Through different methodologies, these tools detect the packages contained in these scanned artifacts on a best effort basis. In the context of this work, a package is defined as functional unit contained in artifacts, whereby it is usually a collection of files forming a library which is imported in the source code. By subsequently matching these packages against different databases such as the NVD, introduced in the foundations chapter, known vulnerabilities and licenses are identified. To give a better idea of these results, figure \ref{fig:bdbaResult} in the appendix shows a snippet returned from the API of Black Duck Binary Analysis (BDBA). The results on their own are useful already and provide interesting data about the above mentioned entities. But it is still loose metadata that lacks context information such as which deployments contain the corresponding entities. Therefore, an additional entity type to conduct further grouping is required. The OCM already introduced such an entity type, the component.\par
To conclude this, from a high level perspective, the important entities are \emph{components}, \emph{sources}, \emph{resources}, \emph{packages} and the information attached to the packages such as vulnerabilities and licenses. To generalize this and abstract away from specific data sources, the entity type representing this types of metadata is called \emph{info snippet}. So these entity types are the basic building blocks for the data model. From here on, it is getting rather complex and abstract. To still keep the explanations tangible, below figure \ref{fig:DataModel} already shows an \emph{entity-relationship model (ERM)} describing the final and universal data model. This may be used as a reference point throughout the following paragraphs discussing the design decisions leading up to the specific entities, relationships and cardinalities.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.65]{datamodel}
	\caption[Data Model]{Data Model \source{Own representation}}
	\label{fig:DataModel}
\end{figure} 

Even though one may not understand the motivation behind every element immediately, the model as a whole should feel quite familiar and intuitive by now. An important notice at this point, the data model is obviously inspired by the OCM. As mentioned above, especially the entity type \emph{component} is lend from the OCM. Since this thesis is written within SAP Gardener, a seamless integration of the this Security and Compliance Data Lake such as described in the previous chapter is of course also a requirement, even though it is not explicit listed. But still, the Security and Compliance Data Lake is designed independent of the OCM. The underlying principles and thereby, the entity and relationship types of the OCM were merely deemed as a suitable approach for an universal data model for software metadata. Thus, in theory it is entirely possible to use a different kind of component model. As an example, if one is able to express the concept of \emph{components} and \emph{artifacts} with the means of the SPDX standard, one could use SPDX instead of OCM to provide this structural information. Or, since SPDX is not optimal for this purpose, one could create and use an own component model, as long as it has the means to express \emph{components} and \emph{artifacts} (There is generally no necessity to distinct between \emph{sources} and \emph{resources}. One could just treat \emph{sources} as \emph{resources}, at the only cost of losing the connecting "is built from"-information between the two entities.).\\\par 

\subsection{Universal Data Model}
The basic entity type \emph{component} is broken down into two distinct entity types, \emph{Component} and \emph{Component Version}. As one may immediately notice, this distinction is done for each of the basic entity types. During the initial design phase, there was a lot of discussion, whether this additional level of aggregation is really necessary. But the final conclusion was, that it always makes sense, especially in regards to extensibility and performance optimization. This will be explicitly addressed and explained in detail several times throughout this and the following sections.\par 
So \emph{Component} is a purely abstract entity. It merely groups all the versions of the same component together. Thereby, the \emph{Component} may provide information about the semantics of this grouping such as whether this \emph{Component} describes a specific deployment or whether it describes all software used by a department. Thus, information that is identical for all versions of this component and would have to be stored redundantly for each \emph{Component Version} otherwise. Naturally, there are multiple \emph{Component Versions} of each \emph{Component}, thus the (n:1)-cardinality here is self-explaining. As established by the previously described grouping semantic of \emph{components}, a \emph{Component Version} may reference multiple other \emph{Component Versions}. As an example, a \emph{Component Version} describing a specific version of a deployment may reference multiple other \emph{Component Versions} such as \emph{Component Versions} describing specific versions of a web server, a service and a database. Reciprocal, a \emph{Component Version} may of course be referenced by multiple \emph{Component Versions}. As an example, a \emph{Component Version} describing a web server may be referenced by several \emph{Component Versions} describing different versions of the same deployment or entirely different deployments. Thus, this is a recursive (n:m)-relationship. There may also be a need to store additional occurrence specific metadata as properties of the \emph{references}. Considering the above example, such occurrence specific metadata may provide information about the usage of the web server within the deployment, hence whether it is used as a HTTP server or as a load balancer. Together, these model elements fulfill requirement R.4 (provide an aggregation level to group sources and resources).\par
Furthermore, a \emph{Component Version} may also reference multiple \emph{Sources} and \emph{Resources}. As an example, the \emph{Resources} comprising the web server and the \emph{Sources} from which the respective \emph{Resources} were built. As before with the recursive relationship of \emph{Component Versions}, the \emph{Sources} and \emph{Resources} may of course also be referenced by multiple \emph{Component Versions}, resulting in a (n:m)-relationship. Again, there may be a need to store additional occurrence specific metadata as properties of the \emph{references}. Specifically, these \emph{references} may be used to store \emph{triage} information. As this \emph{reference} describes the usage context of the \emph{Artifact}, one may decide whether a copyleft license is or is not acceptable here. The relationships between \emph{Source} and \emph{Source Version} as well as between \emph{Resource} and \emph{Resource Version} is similar to the relationship between \emph{Component} and \emph{Component Versions}. But the abstract \emph{Source} and \emph{Resource} entities actually do have a concrete purpose but preventing redundant storage of certain properties. In this case, these abstract entities may have properties to store \emph{triage policies}. As an example, one may store that a specific vulnerability may be ignored for the usage of \emph{Resource Versions} v1.0.0 to v1.2.3 of a respective \emph{Resource} within \emph{Component Versions} v1.4.2 to v1.4.12 of a specific \emph{Component}. The \emph{references} and the abstract \emph{Source} and \emph{Resource} entities thereby enable the fulfillment of requirement R.6 (enable users to perform assessments).\par 
\emph{Resource Versions} may also reference the \emph{Source Versions} they are built from. A \emph{Resource Version} may be built from multiple \emph{Source Versions} and a \emph{Source Version} may be used to build multiple \emph{Resource Versions}. This also results in a (n:m)-relationship.\par
Since \emph{Artifacts} are comprised of \emph{Package Versions} and the same \emph{Package Version} may occur in multiple \emph{Artifacts}, both \emph{Source Version} as well as \emph{Resource Version} have a (n:m)-relationship to \emph{Package Version}. Again, there may be a need to store additional occurrence specific metadata as properties of the \emph{is comprised of} relationship.\par
The relationship between \emph{Package} and \emph{Package Version} is again similar to the relationship between \emph{Component} and \emph{Component Versions}. But \emph{packages} are broken down even further into three different entity types and thereby three aggregate levels. The third level, called \emph{Native Package Version}, exists due to the fact, that different scanning tools may provide different of information for the same \emph{Package Version}. \emph{Native Package Version} is in this respect also not a classical entity type. It is more like an interface entity type as it actually represents an arbitrary number of concrete entity types such as for example "BDBA Package Version" and "Mend Package Version". But as new scanning tools may be added with time, the complete set of tools cannot be known upfront. It is also in so far an interface entity type, as all concrete entity types will have to provide a specific predefined set of attributes that may be merged in the \emph{Package Version} aggregate level. But additionally to this predefined set of attributes they may also have an arbitrary number of attributes that are native for this tool and not considered for merging. This will be discussed in further detail in the next section. In total, the third aggregate level, \emph{Native Package Version} enables requirement R.2 (storing metadata from different sources without aggregation) and the merging on the second aggregate level, \emph{Package Version}, enables R.3 (providing metadata from different data sources with aggregation).\par
\emph{Package Versions} frequently \emph{depend on} other \emph{Package Versions} and so on. This may lead to quite long chains of dependencies. This has to be kept in mind, as these transitive dependencies are also relevant when trying to answer the question, whether a certain \emph{Artifact} or \emph{Component} contains a specific \emph{Package} such as Log4J, and its corresponding vulnerabilities.\par
Finally there is the \emph{Info Snippet} entity type. This is also more of an interface entity type, just as \emph{Native Package Version}. This is highlighted by the dashed shapes in \ref{fig:DataModel}. This interface entity types represent concrete entity types such as "vulnerability" and "license" but also "build information". Again, the whole set of entity types cannot be known upfront. Contrary to the \emph{Native Package Version} interface entity type, these do not even need to have a predefined set of attributes as they provide entirely different kinds of information. For \emph{Info Snippets}, the concrete relationships and the respective cardinalities depend on the concrete \emph{Info Snippet} type. Vulnerabilities and licenses are commonly contained in multiple \emph{Package Versions} and \emph{Package Versions} usually contain multiple vulnerabilities and licenses, leading to a (n:m)-relationship. Build information on the other hand is usually attached to exactly one specific \emph{Resource Version} and a \emph{Resource Version} has one or multiple build information depending on whether or not historic build data is stored, leading to a (1:1) or (n:1)-relationship. This rather loose definition of \emph{Info Snippet} and \emph{Native Package Version} together enable R.1 (consuming and storing metadata from multiple different data sources).\par

\subsection{Complementary Meta Data Model}
The previous section rarely mentions properties and the data model in figure \ref{fig:DataModel} does not show any properties at all. This is on purpose and has two major reasons.\par 
Firstly, as already mentioned in the beginning of the data model section, the Security and Compliance Data Lake is designed to be independent of the OCM and open to other component models. This would hardly be possible if the data model would define fixed predefined properties for each entity type.\par
Secondly, the different scanning tools provide a wide range of information about packages and other data sources but scanning tools may be added. It is therefore practically impossible to foresee what information from the scans may be needed and should get a property in the data model. Besides, these may vary depending on the user of the Security and Compliance Data Lake.\\

\noindent\textbf{Insights into the Development Process}\\
At this point, some insight into the development process may be beneficial to understanding the design decisions. The scope of this central data store was initially much narrower. The first PoC was actually strictly bound to the OCM. Therefore, the data model predefined the properties of \emph{component} and \emph{artifact} entity types. As it was bound to the OCM, there was no issue in doing so. But the data model also predefined the properties of the \emph{package} entity types. In fact, the third aggregate level for \emph{packages}, \emph{Native Package Version} did not exist at all. Instead, the \emph{Package Version} entity type had a set of properties that was hoped to be common and possible to harmonize throughout all prospective data sources. To define this set of properties, the API documentations of different scanning tools were analyzed for the common and most important properties, especially the ones of BDBA and Mend \cite{MendAPI}. Additionally, to get a better understanding, both scanners were used on some artifacts to get some sample data. Then, the provided attributes were narrowed down and some interviews with developers were conducted. This was a huge effort, as this was such an important decision. Also, instead of having the \emph{Info Snippet} entity type, there was only a \emph{Vulnerability} and a \emph{License} entity type whose set of properties was defined in the same process. Besides the fact that there was already a substantial amount of disagreement between different developers which attributes were actually required, by the time the PoC was finished, several new use cases were discovered that required additional properties and even entire additional entities to provide other information than about vulnerabilities or licenses.\par
This lead to a change in perspective, interpreting the task of defining the right entity types with the right properties rather as a task to make the entire application extensible regarding respective entity types and properties. But this flexibility and extensibility comes at the cost of a highly increased overall complexity. Besides the remodeling of the data model, it also required a completely new architecture and completely different implementation. This was primarily to also enable other \emph{Info Snippet} entity types and to solve issue of having to agree on a set of properties for \emph{Package Version} upfront. It was not until this functionality was almost completely implemented, that it was noticed that the same approach to dynamically creating \emph{Info Snippet} types and \emph{Native Package Versions} could also be applied to allowing to dynamically set the properties of other the other entity types. Thus, only after that the scope widened drastically, also allowing other component models. Consequently, as a kind of disclaimer, the order presented here is not exactly chronological as it hides a complete development iteration leading up to the final concepts.\\\par

To enable the dynamic definition of the properties of the entity types and the creation of entirely new entity types, there is an additional kind of complementary meta data model. On its own, the following may be hard to grasp and to imagine how this may be implemented, as its becoming very abstract at this point. But throughout the following chapters, everything will fall into place. The meta data model itself is rather simple.\par 
In essence, it requires every entity to differentiate between \emph{Identity} and \emph{Attribute} properties. Thus, to differentiate between properties that are capable of uniquely identifying the entity in a list of entities of the same entity type and properties that are only informational. In the case of \emph{Native Package Version} it requires to additionally differentiate between \emph{Attributes} and \emph{Native Attributes}. So between the common properties that all data sources should provide and will be merged on \emph{Package Version} level and the properties that are native to the respective data source and will consequently not be considered on the higher \emph{Package Version} aggregate level. The below figure \ref{fig:MetaDataModel} is an extension of \ref{fig:DataModel} and tries illustrate this concept.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{extendeddatamodel}
	\caption[Data Model]{Data Model \source{Own representation}}
	\label{fig:DataModel}
\end{figure} 

These meta properties are also illustrated with dashed shapes, as the actual properties are also not known ahead of time. Consequently, \emph{Identity} and \emph{Attributes} may be arbitrary sets of properties. But there are two restrictions. The \emph{Identities} of \emph{Component Version}, \emph{Source Version} and \emph{Resource Version} have to contain a \emph{version} property and the \emph{Identity} of the corresponding grouping, not versioned entity type, so in case of \emph{Component Version} the \emph{Component} entity type, has to be exactly the same but  this \emph{version} property. This is a necessary requirement in order to being able to identify all versions of the same \emph{Component}.\par
Furthermore, the \emph{Identity} of \emph{Native Package Version} has to contain a \emph{name} and also a \emph{version} property. The \emph{version} is again required to identify all versions of the same \emph{Package}. The \emph{name} is included to maintain the searchability.

\subsection{Application of the Data Model}
The previous descriptions of the data model have become quite abstract, especially in combination with the meta data model. Therefore, this section discusses how this data model could be applied. As reference component model, the OCM is used and as reference data source, BDBA is used. This thereby also reveals a problem that has to be faced when applying this data model to a real world use case. The figure \ref{refdatamodel} below shows the corresponding data model instance.\par

 


\subsubsection{Artifact Identity Problem}
\subsubsection{Open Component Model Approach}
\subsubsection{Single Source of Truth Approach}

 



At this point, there is an contradiction between the OCM and the specification of the data model. The prerequisite to being able to group all \emph{Component Versions} to a \emph{Component} is a common globally unique identifier, the component name, which allows to identify all \emph{Component Versions} as versions of a specific \emph{Component} in the first place. But as described in the OCM section, the \emph{Source Versions} and \emph{Resource Versions} only have a component version-local identity. So the same physical \emph{Resource} could be identified by a different name in each \emph{Component Version}. Thus, there is no way of identifying whether \emph{Resource Versions} referenced by two different \emph{Component Versions} are the same \emph{Resource}. In fact, even if \emph{Resource Versions} referenced by two different \emph{Component Versions} do have exactly the component version-local identity (name, version and extra identity), still the only way to determine whether they are the same \emph{Resource Version} is to calculate and compare normalized digests, since even the access properties may differ. In practice, the usage of \emph{Source Versions} and \emph{Resource Versions} is that the identity is at least component-local. Thus, \emph{Source Versions} and \emph{Resource Versions} of different \emph{Component Versions} of the same \emph{Component} with the same local identity (name, version and extra identity) can be assumed to be the same. Even more, in practice, if the name and extra identity of \emph{Source Versions} or \emph{Resource Versions} of different \emph{Component Versions} of the same \emph{Component} are the same, they may be assumed to be the same. Hence, when consuming the structural information from the OCM, the relationship between \emph{Component Version} and \emph{Source Version} and \emph{Resource Version} has an actual (1:n)-cardinality. Consequently, triaging policies on \emph{Sources} and \emph{Resources} are automatically bound to the context of a \emph{Component} and multiple \emph{Source Versions} with different identities (component name, source name, source version and extra identity) may refer to the same physical source code and multiple \emph{Resource Versions} may respectively refer to the same physical binaries. But since they do in fact refer to the same physical source code or binaries, they have the same scan results. Therefore, while this is not completely clean and satisfying, its still functional.\par 






\section{Terminology}
The System Design section makes use of a lot of heavily overloaded words which may lead to confusion and make it quite difficult to follow. To prevent this, the meaning of those ambiguous words will be specified for this context in the following:\\

\noindent
\textbf{Component:} A component is an abstract entity describing a dedicated usage context or meaning for provided software, as defined by the OCM \cite{OCMSpec}.\\
\textbf{Artifact:} An artifact is an umbrella term for sources and resources. Thus, the term refers to the actual source code or binaries.\\
\textbf{Package:} A package is a functional unit contained in an artifact. In practice, a package is usually a collection of files, forming a library which is imported in the source code. 

\section{Data Model}
The basic entities relevant in the software supply chain are artifacts, thus sources and resources, and the packages comprising these artifacts.\par
Compliance scanners usually scan entire source code repositories or binaries. Through different methodologies, these tools detect the packages contained in these scanned artifacts. By subsequently matching these packages against different databases such as the NVD introduced in the foundations chapter, known vulnerabilities and licenses are identified. To give a better idea of these results, figure \ref{fig:bdbaResult} in the appendix shows a snippet returned from the API of Black Duck Binary Analysis (BDBA).\par 
The results on their own are useful already and provide interesting data about the above mentioned entities. But it is still loose metadata that lacks context information such as which deployments contain the corresponding entities. Therefore, an additional entity to conduct further grouping is required. The OCM already introduced such an entity, the component.\\\\
To conclude this, from a high level perspective, the important entities are \emph{components}, \emph{sources}, \emph{resources}, \emph{packages} and the information attached to the packages such as vulnerabilities and licenses. To generalize this and abstract away from specific tools, the entity representing this types of metadata is called \emph{info snippet}.\par 
So these entities are the basic building blocks for the data model. From here on, it is getting rather complex. A section listing the requirements beforehand was omitted on purpose, because they would have been very abstract and hard to follow. Instead, the requirements are now mentioned along the corresponding design decisions. To keep it even more tangible, below figure \ref{fig:DataModel} shows an entity-relationship model (ERM) describing the final data model. This may now be used as an anchor point throughout the following paragraphs discussing the requirements and design decisions leading up to the specific entities, relationships and cardinalities.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.65]{datamodel}
	\caption[Data Model]{Data Model \source{Own representation}}
	\label{fig:DataModel}
\end{figure} 




\section{Database}
\section{API}
Tool agnostic, how does the api look like to abstract away from different tools

\section{Requirements}
Before actually going into the details of the systems design, the requirements have to be specified, since they are at the core of every design decision.  

\subsection{Functional Requirements}
\begin{xltabular}{\linewidth}{|l|X|l|}
	\hline \hline \hline \rowcolor{lightgray}\multicolumn{3}{|l|}{\cellcolor{lightgray}{\textbf{Requirements}}} \\ \hline \rowcolor{lightgray} \textbf{Ref.\#} & \textbf{Functionality} & \textbf{Prio.}\\ \hline
	\endfirsthead
	
	\hline \hline \hline \rowcolor{lightgray}\multicolumn{3}{|l|}{\cellcolor{lightgray}{\textbf{Requirements}}} \\ \hline \rowcolor{lightgray} \textbf{Ref.\#} & \textbf{Functionality} & \textbf{Prio.}\\ \hline
	\endhead
	
	\hline \multicolumn{3}{|r|}{{Continued on next page}} \\ \hline
	\endfoot
	
	\hline \caption{Requirements} \label{Tab:Requirements}
	\endlastfoot
	
	R.1 & The SCDL shall be able to consume and store metadata from multiple different data sources.\newline\newline
	The SCDL shall be able to work with any kind of metadata about software components.	Therefore, it has to be able to handle multiple different scanning tools, as well as other kinds of data sources like build tools. As an example, it might have to consume data from BDBA, Mend but perhaps also Jenkins. Thus, one has to consider that besides vulnerabilities and licenses, a variety of other metadata types may need to be added in the future. & 1\\
	\hline
	R.2 & The SCDL shall store the metadata from different data sources without aggregation\footnotemark{}.\newline\newline
	Different tools that generally serve the same purpose may provide similar information. As an example, BDBA and Mend are both SCA tools and therefore provide overlapping results. To ensure that no data is lost, this information shall not be combined and aggregated before storing.
	\footnotetext{\textit{aggregation} in this context means to merge the data about a package of a BDBA scan and a Mend scan to a single package entity instance before storing} & 1\\
	\hline
	R.3 & The SCDL shall provide the metadata from different data sources with aggregation\footnotemark[\value{footnote}].\newline\newline
	As mentioned before, to ensure no data is lost, the data from different data sources shall be stored without aggregation. Anyway, to be consumed by a user, this data shall be aggregated. As an example, when querying all packages contained in a specific resource, the result returned by the SCDL shall not contain the same package twice in different representations, if it was identified by BDBA and by Mend. Instead, it shall contain an aggregated representation of the package. Thus, some kind of aggregation layer is needed which provides transparency regarding the data sources. & 1\\
	\hline
	R.4 & The SCDL shall provide a level of aggregation\footnotemark{} to group sources and resources.\newline\newline
	\footnotetext{\textit{aggregation} in this context refers to the "whole/part" semantic of the word \cite{UML}. Thus, since resources and sources are comprised of packages, they are both aggregations of packages. On a model level, the same applies for the relationships between packages and vulnerabilities or licenses as well as between entire deployments and the deployed resources.}
	As pointed out before, one problem also with SBOMs is the disconnection of the artifact metadata and the deployment information. To bridge this gap, an additional aggregation level for grouping artifacts is necessary. As an example, this additional aggregation level shall enable to group all resources contained in a specific deployment. & 1\\
	\hline
	R.5 & The SCDL shall enable users to query the metadata on different levels of aggregation\footnotemark[\value{footnote}]\newline\newline
	As an example, a user shall be able to query for all vulnerabilities in a specific resource, thus query on the aggregate level of resources. But a user shall also be able to query for all vulnerabilities in an entire specific deployment, thus querying on the aggregate level of deployments (querying on this level of aggregation enables to answer where Log4J is deployed). & 1\\
	\hline
	R.6 & The SCDL shall enable users to perform assessments.\newline\newline
	The relevance of specific pieces of information such as vulnerabilities or licenses depends on the use case. As an example, while the internal usage of an altered OSS with a copyleft license is lawful, the distribution is not. Therefore, a possibility has to be provided to assess such pieces of information in the context of their occurrence. & 1\\
	\hline
	R.7 & The SCDL shall provide common data aggregation and filter functions for the queries.\newline\newline
	As an example, a user shall be able to filter for the vulnerability with the highest CVSS within a resource or shall be able to get the count of vulnerabilities within a resource. & 2\\
	\hline
	R.8 & The SCDL shall enable users to query the metadata in the common SBOM formats.\newline\newline
	In order to be able to fulfill governmental requirements of the executive order mentioned in the Software Bill of Materials section, the SCDL has to provide a way to to query the metadata in the common SBOM formats. As an example, a user shall be able to query the SPDX document for a specific resource. & 2\\
\end{xltabular}

\subsection{Non-functional Requirements}
Since this shall be a prototypical implementation, there is a strong focus on fulfilling the functional requirements. Anyway, performance definitely has to be considered in design decisions already, especially since the SCDL shall serve as the backend for a dashboard web application.
Scalable (Wie viele Daten fallen im Gardener an? Wie lange wird das gut gehen? Datenarchivierung) --> PoC aufbauen und dann nachschauen! 

~200MB/Day pro Scanner

