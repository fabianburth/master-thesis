%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Master's Thesis          %	    										
% Fabian Burth, 2022-08-01 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\npchapter{State of the Art}
This chapter gives an overview of the state of the art approach of integrating security and compliance measures into the \textit{software development life cycle (SDLC)}. That includes a brief introduction of practices such as \textit{Continuous Integration and Continuous Delivery (CI/CD)} and \textit{DevOps}. In the end, the limitations of this current approach are discussed, thereby further motivating this thesis. 

\section{Software Development Life Cycle}
Since the emerging of cloud technologies, the software industry, especially also the major software companies like Microsoft, Amazon and SAP shifted their business model from \textit{software as a product (SaaP)} to \textit{software as a service (SaaS)}. This gave companies the opportunity to frequently release updates without adhering to a rigid distribution cycle. So they could react to feedback much faster and improve their software continuously \cite{DevSecOps, ContinuousSecurity}.\par
To keep up with this new pace, the SDLC had to be adjusted as well. So developers started to \emph{continuously integrate (CI)} their code after they conducted some changes while at the same time automatically testing the software. Thus, always keeping it up to date and in a shippable state.\par
As an extension of that idea, the software is also automatically and \emph{continuously delivered (CD)} to testing or even production environments, accelerating the process even more \cite{CICD}.\par 
A quick side note - also after reviewing some literature, there seems to be some disagreement whether to distinguish continuous delivery and continuous deployment \cite{CICD, DevSecOps}. The articles and papers that do differentiate define continuous delivery as automatically delivering to production-like environments for evaluation and testing. But there are still some manual steps necessary to actually deploy into production or customer environments \cite{CICD, CICDDebate1, SecureCloudApplications}.\par
\emph{DevOps}, a combination of "development" and "operations", is a practice which also resulted from this change of the SDLC. Since the concept of CI/CD merges areas of development and operations, the corresponding roles were combined to reduce communication overhead and misunderstandings \cite{SecureCloudApplications}. DevOps is usually reliable for the setup and maintenance of the \emph{CI/CD Pipeline}. Hence, they automate the steps required for the continuous integration and delivery, such as building and testing the software after a developer integrated his code changes and moving it to the next step after the previous tests and checks are passed. 

\section{Integrating Security and Compliance Measures}
In practice, in order to conduct CI/CD, DevOps usually sets up a so called \emph{CI/CD Pipeline} for the project. This includes a set of tools to automate build, test and deployment of the software. The most popular and widely used example is Jenkins \cite{jenkins}. But essentially, CI/CD pipelines are just simple stage-gate systems. Thus, the process is divided into a number of stages. Between each stage is a quality gate, such as a set of automated tests. The software has to pass this quality gate in order to being able to move to the next stage \cite{StageGate}.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{cicd}
	\caption[CI/CD Pipeline as Stage-Gate System]{CI/CD Pipeline as Stage-Gate System \source{Based on \cite{StageGate}}}
	\label{fig:CI/CD Pipeline}
\end{figure}

Figure \ref{fig:CI/CD Pipeline} is an abstract technology agnostic illustration of a CI/CD pipeline as a stage-gate system. This also shows the currently established state of the art approach to reduce software supply chain risks such as problematic licenses or vulnerabilities in dependencies. Security and compliance scans are conducted as part of the CI/CD pipeline. The results of these scans may also be triaged before they are finally checked against a defined set of policies, forming another quality gate \emph{before delivery}.\par 
Triage is a term derived from medicine where it describes the classification and prioritization of patients if there are not enough resources to treat them all immediately. In software testing or scanning, it refers correspondingly to the process of (re-)rating and (re-)classifying issues found during the scan in the context of its occurrence within the particular project. Common issues include problematic licenses or vulnerabilities. For the latter, the triage may be done based on the CVSS introduced in the previous chapter. Policies may then prevent shipping of software that contains vulnerabilities with a CVSS that exceeds a certain threshold or specific blacklisted licenses.\\\\
Unfortunately, there is not a lot of academic literature on this specific topic to back up the claim that this is the state of the art approach. Therefore, for the purpose of this work, the offerings of the major vendors of corresponding tools, hence \emph{Software Composition Analysis (SCA)} tools were deemed as a suitable source. SCA tools usually scan source code repositories or binary files, analyze the dependencies and match them against vulnerability or license databases.\par
According to a a market research of Forrester from 2021, based on a combination of market presence, current offering and strategy, these are Mend (formerly known as WhiteSource), Synopsys, Sonatype and Snyk \cite{ForresterSCA}. All of these tools advertise their smooth integration into the CI/CD pipeline, to be more precise into the version control systems and build tools, as another quality gate \cite{MendIntegration, BDBAIntegration, SonatypeIntegration, SnykIntegration}. This should in general be a suitable representation of what the industry is requesting, although it is to mention that most of these vendors also offer other integration options. Another popular one for the SCA tools that scan binary files are the artifact repositories.\par 
Additionally, it is undeniable that it makes a lot of sense to integrate these scans as another quality gate as part of the CI/CD pipelines into the repositories. At this point, all the files of a project are easy accessible for these tools. Besides, it is a commonly known principle in process management, that a defect becomes more expensive the later it is found.\par 
 
\section{Producing Software Bill of Materials}
As a solution to increasing the supply chain transparency, the US government decided that analogous to other industries their software vendors have to provide SBOMs. Subsequently, the NTIA conducted research and published several documents on the topic. One of those being the minimum elements for SBOMs discussed in the foundations chapter. As mentioned there, one of the minimum requirements is automation support. Therefore, the NTIA conducted a survey of existing SBOM formats and standards considering how to integrate them into the SDLC \cite{SBOMSurvey}.\par
In this document the NTIA also suggests to leverage existing tools such as version control systems, build tools, code scanners and binary analysis tools to generate SBOMs \cite{SBOMSurvey}. Thus, the tools already integrated into the CI/CD pipelines as established in the previous paragraphs. Since these SCA tools obtain some of the most important SBOM information like dependencies and licenses anyway, they can also provide these results in a specific SBOM format. Therefore, all of the previously mentioned SCA tool vendors quickly adjusted and do provide such options now.

\section{Limitations}
This approach is intuitive and might be sufficient for the scope of a single development team which deploys its software to a single infrastructure. But the limitations really start to show if this scope is exceeded.\par
While the quality gate approach is not wrong, it is insufficient. Once the software passed the corresponding quality gate, there are no more scans. But new vulnerabilities in one of the dependencies may still be discovered after this point in time. These would then go undetected until the respective quality gate is reached again with the next version of the software. Besides, the main software that is being developed might depend on additional third party software at runtime. Consequently, this third party software has to be delivered alongside the main software. But the third party software might never pass any of the companies CI/CD pipelines and thus never pass the quality gates. Therefore, it would never be scanned and its vulnerabilities would again go undetected. Furthermore, with the quality gate approach, the scans are frequently also treated as such. Thus, developers hope for their code to pass the scan and discard the results of it does. Finally, with this approach, each development team has to configure, integrate and maintain this scanning tools on their own in each of their pipelines. In larger companies, this often also leads multiple opinionated CI/CD pipelines with additional point to point integration for such compliance tools and individual reporting dashboards as an attempt to overcome the existing limitations of this approach.\par
To solve these issues, the \textit{compliance scans have to be decoupled from the CI/CD pipeline}. A quite popular solution to this is to integrate the them with the artifact repositories. Thereby, even vulnerabilities in components that do not pass an internal CI/CD pipeline are detected. But of course this only works with the binary scanners, which usually provide less precise results. Besides, this might also get tedious and end up in multiple point to point solutions in cases where the artifacts are distributed over several different artifact repositories. Another possible option is based on the production of SBOMs during the compliance scans. When initially passing the quality gate, an SBOM can be generated, which may be used as access point to conduct further compliance scans. The tools of Mend and Synopsys already provide such an option. The problem here is, that the components in the generated SBOMs usually have tool specific identifiers. Therefore, each tool can only conduct scans based on their own SBOM. So both of this approaches are not ideal, and even if they were, there would still be a problem.\par
One might imagine a situation where the artifacts, which comprise the final product and are built from different internal as well as third party repositories, are stored in multiple different artifact repositories. From these repositories the artifacts are deployed into multiple different production environments. But naturally, there are different versions of the final product which are comprised of different versions of the artifacts. Given a scan detects a vulnerability in a specific version of an artifact in the artifact repository, how would one go about telling which environments contain this particular version of the artifact? Of course, the information about which product version contains what version of the artifacts is stored somewhere, the SBOMs for example. And the information where this product version is deployed is most certainly also stored somewhere else. But ultimately, there is no easy way to answer this question without skimming through this different data sources. This concludes the final and central issue.\par
\emph{Software metadata is distributed over the entire software development life cycle. Thus, there is a need for a central application for storing and querying software metadata.}


%Collect metadata and life cycle information about the software
%
%- SBOMs contain the dependencies. these dependencies may change over time, which also makes checks against vulnerability databases incomplete
%- difficult to collect state of deployments
%- shift left into repository and version control





%In a NTIA survey about SBOMs, they also studied the different data sources in the software life cycle for producing SBOMs. Back up our claim


%\section{Common Approach of the Industry}
%\section{Current Approach at SAP Gardener}
%\section{Problems of Existing Approaches}