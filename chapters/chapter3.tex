%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Master's Thesis          %	    										
% Fabian Burth, 2022-08-01 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\npchapter{State of the Art}
This chapter gives an overview of the state of the art approach of integrating security and compliance measures into the \textit{software development life cycle (SDLC)}. That includes a brief introduction of practices such as \textit{Continuous Integration and Continuous Delivery (CI/CD)} and \textit{DevOps}. In the end, the limitations of this current approach are discussed, thereby further motivating this thesis. 

\section{Software Development Life Cycle}
Since the emerging of cloud technologies, the software industry, especially the major software companies like Microsoft, Amazon and SAP, shifted their business model from \textit{software as a product (SaaP)} to \textit{software as a service (SaaS)}. This gives companies the opportunity to frequently release updates without adhering to a rigid distribution cycle. In consequence, they can react to feedback much faster and improve their software continuously \cite{DevSecOps, ContinuousSecurity}.\par
To keep up with this new pace, the SDLC had to be adjusted as well. Developers started to \emph{continuously integrate (CI)} their code after they conducted some changes, while at the same time automatically testing the software. Thus, they were always keeping their code up to date and in a shippable state.\par
As an extension of that idea, the software is also automatically and \emph{continuously delivered (CD)} to testing or even production environments, accelerating the process even more \cite{CICD}.\par 
A quick side note -- also after reviewing some literature, there seems to be some disagreement whether to distinguish continuous delivery and continuous deployment \cite{CICD, DevSecOps}. The articles and papers that do differentiate between these terms, define continuous delivery as automatically delivering to production-like environments for evaluation and testing. But there are still manual steps necessary to actually deploy to production or customer environments \cite{CICD, CICDDebate1, SecureCloudApplications}.\par
\emph{DevOps}, a combination of ''development'' and ''operations'', is a practice which also resulted from this change of the SDLC. Since the concept of CI/CD merges areas of development and operations, the corresponding roles were combined to reduce communication overhead and misunderstandings \cite{SecureCloudApplications}. DevOps is usually reliable for the setup and maintenance of the \emph{CI/CD pipeline}. Hence, they automate the steps required for the continuous integration and delivery, such as building and testing the software after a developer integrated his code changes and moving it to the next step after the previous tests and checks are passed. 

\section{Integrating Security and Compliance Measures} \label{sec:Integrating Security and Compliance Measures}
In practice, in order to conduct CI/CD, DevOps usually sets up a so called \emph{CI/CD pipeline} for the project. This includes a set of tools to automate build, test and deployment of the software. The most popular and widely used example is Jenkins \cite{jenkins}. But essentially, CI/CD pipelines are just simple stage-gate systems. Thus, the process is divided into a number of stages. Between each stage is a quality gate, such as a set of automated tests. The software has to pass this quality gate in order to being able to move to the next stage \cite{StageGate}.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{cicd}
	\caption[CI/CD Pipeline as Stage-Gate System]{CI/CD Pipeline as Stage-Gate System \source{Based on \cite{StageGate}}}
	\label{fig:CI/CD Pipeline}
\end{figure}

Figure \ref{fig:CI/CD Pipeline} is an abstract technology-agnostic illustration of a CI/CD pipeline as a stage-gate system. The concept represented in this illustration also shows the currently established state of the art approach to reduce software supply chain risks such as problematic licenses or vulnerabilities in dependencies. Security and compliance scans are conducted as part of the CI/CD pipeline. The results of these scans may also be triaged before they are finally checked against a defined set of policies, forming another quality gate \emph{before delivery}.\par 
Triage is a term derived from medicine where it describes the classification and prioritization of patients if there are not enough resources to treat them all immediately. In software testing or scanning, it refers correspondingly to the process of (re-)rating and (re-)classifying issues found during the scan in the context of its occurrence within the particular project. Common issues include problematic licenses and vulnerabilities. For the latter, the triage may be done based on the CVSS introduced in the previous chapter. Policies may then prevent shipping of software that contains vulnerabilities with a CVSS that exceeds a certain threshold or specific blacklisted licenses.\\

Unfortunately, there is not a lot of academic literature on this specific topic to back up the claim that this is the state of the art approach. Therefore, for the purpose of this work, the offerings of the major vendors of the security and compliance scanning tools were deemed a suitable source.\par
The security and compliance scanning tools used to retrieve this kind of information are called \emph{Software Composition Analysis (SCA)} tools. Those tools typically scan entire software \emph{artifacts}, thus, source code projects or binaries, and provide information about dependencies to other software projects, in other words, about libraries and frameworks used within the artifact. These software projects composing an artifact are generally referred to as \emph{packages}. The packages are then matched against different databases such as the NVD to check for and retrieve known vulnerabilities and licenses.\par
According to a market research of Forrester from 2021, based on a combination of market presence, current offering and strategy, those major vendors are Mend (formerly known as WhiteSource), Synopsys, Sonatype and Snyk \cite{ForresterSCA}. All of these tools advertise their smooth integration into the CI/CD pipeline, to be more precise, into the source code repositories and build tools, as another quality gate \cite{MendIntegration, BDBAIntegration, SonatypeIntegration, SnykIntegration}. This should generally be a suitable representation of what the industry is requesting, although it is to mention that most of these vendors also offer other integration options. Another popular one for the SCA tools that scan binaries are the artifact repositories.\\

\emph{After all, the reason for integrating these scans as another quality gate as part of the CI/CD pipeline into the repositories is evident.  There, the source files or alternatively the binary files are conveniently accessible for these tools.}

\section{Producing Software Bill of Materials}
As a solution to increasing the supply chain transparency, the US government decided that, analogous to other industries, their software vendors have to provide SBOMs. Subsequently, the NTIA conducted research and published several documents on the topic. One of those is the minimum elements for SBOMs discussed in the foundations chapter. As mentioned there, one of the minimum requirements is automation support. Therefore, the NTIA conducted a survey of existing SBOM formats and standards considering how to integrate them into the SDLC \cite{SBOMSurvey}.\par
As part of this survey, the NTIA suggests to leverage existing tools such as version control systems, build tools, code scanners and binary analysis tools -- thus, the tools already integrated into the CI/CD pipelines -- to generate SBOMs \cite{SBOMSurvey}.\par
The previous section explained that SCA tools retrieve the decomposition information about artifacts, in other words, the packages composing an artifact, as well as their respective vulnerabilities and licenses. Since this is the most important SBOM information, the SCA tools can also directly provide these results in a specific standardized SBOM format. Therefore, all of the mentioned SCA tool vendors quickly adjusted and provide such options now.

\section{Limitations} \label{sec:Limitations}
The CI/CD integration is intuitive and might be sufficient for the scope of a single development team which deploys its software to a single infrastructure. But if this scope is exceeded, there are several serious limitations regarding the following aspects:\\

\noindent\textbf{Synchronous Scans:} Once a software artifact has passed the corresponding quality gate within the CI/CD pipeline, it will not be scanned again. But new vulnerabilities in one of the dependencies may still be discovered after this point in time (e.g. after the software is delivered). Consequently, these vulnerabilities remain undetected until the respective quality gate is reached again with the next version of the artifact.\\

\noindent\textbf{Third Party Artifacts:} The software system that is being developed might depend on additional third party artifacts at runtime. These third party artifacts have to be delivered alongside the internally developed artifacts. Since the third party artifacts are not developed internally, these may never pass one of the companies' internal CI/CD pipelines and, thus, never pass the quality gates. Therefore, these artifacts may never be scanned and their vulnerabilities may again remain undetected.\\

\noindent\textbf{Quality Gate Mentality:} With the quality gate approach, the scans are frequently only treated as gates. Thus, developers hope for their code or binary to pass the scan and discard the results if it does.\\
 
\noindent\textbf{Point to Point Integration:} With this approach, each development team has to configure, integrate and maintain the scanning tools on their own in each of their pipelines. In larger companies, this often leads to multiple opinionated CI/CD pipelines with additional point to point integration for such SCA tools and to individual corresponding reporting dashboards.\\

\noindent\textbf{Distribution of Scan Results over Multiple Technologies:} Development teams that use several different SCA tools, for example, Synopsys' \emph{Black Duck Binary Analysis (BDBA)} for binary scanning and Mend's tool for source code scanning, the results are distributed over multiple different technologies with \emph{individual formats and identifiers}.\\

\noindent There are popular alternatives to decouple the scans from the CI/CD pipeline. Thereby, these alternatives solve especially the first three limitations, but also introduce other problems and are still insufficient:\\

\noindent\textbf{Scan Integration into Artifact Repositories:} Through integrating the scanning tools into the artifact repositories, even vulnerabilities in artifacts that do not pass an internal CI/CD pipeline are detected. But this \emph{only works with the binary scanners} which usually provide less precise results. Besides, this might get tedious and end up in multiple \emph{point to point solutions} as well, in cases where the artifacts are distributed over several different artifact repositories.\\

\noindent\textbf{Scanning based on SBOMs:} 
Another possible option is based on the production of SBOMs during the compliance scans. When initially passing the quality gate, a SBOM can be generated, which may be used as access point to conduct further compliance scans. The tools of Mend and Synopsys already provide such an option. The problem here is that the components in the generated SBOMs usually have \emph{tool specific identifiers}. Therefore, each tool can only conduct scans based on their own SBOM.\\

\noindent So both of these approaches are not ideal. The following example makes the practical implications of these limitations more tangible.\par
Assuming there is a situation where the artifacts, which compose the final software system and are built from different internal as well as third party source code repositories, are stored in multiple different artifact repositories. From these artifact repositories the artifacts are deployed into multiple different environments (naturally, different environments might be comprised of different versions of the artifacts).\par
Given a vulnerability in specific versions of a popular OSS package is discovered, how could a question about which environments contain these particular vulnerable versions of the package be answered?\par
Assuming regular scans of the artifacts are conducted through one of the alternative approaches allowing scanning asynchronously from the CI/CD pipeline, the result of the scanning tools or the respective generated SBOMs of the artifact versions contain this information. Furthermore, the information to which environments the artifacts are deployed is usually stored in respective DevOps repositories. But ultimately, there is no easy way to answer this question without tediously searching through all these different data sources.\par
So the above identified limitation of these approaches, that the scan results are distributed over multiple technologies, may be extended and generalized to:\par
\textbf{Software metadata is distributed over the entire software development life cycle.}\\

To solve the described problems, it is necessary to find a technology-agnostic way to decouple the scans from the CI/CD pipeline and to bridge the gap between dedicated software installations over the effectively used software artifacts down to the various packages these artifacts are composed of and for which, finally, the desired metadata (e.g vulnerabilities or licenses) is provided. To achieve this, the structural information together with the finally desired metadata and all the relations in between can be stored in a central structured data store -- the \emph{Security and Compliance Data Lake}.\par
This requires a uniform identification and access scheme, used along the complete development life cycle to bind all this information together. These schemes can then be used by all kinds of tools around such an ecosystem to feed data into the \emph{Data Lake}, as well as to provide a consolidated view on the complete set of covered software, regardless of their technology or build pipeline environment.

